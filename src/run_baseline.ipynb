{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "import sacrebleu\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ----------------- Config -----------------\n",
    "DATA_DIR = \"wikilarge\"\n",
    "MODEL_NAME = \"t5-base\"\n",
    "OUTPUT_DIR = \"t5_base_wikilarge_baseline\"\n",
    "\n",
    "MAX_SOURCE_LENGTH = 128\n",
    "MAX_TARGET_LENGTH = 128\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "EVAL_BATCH_SIZE = 32\n",
    "LR = 5e-5\n",
    "\n",
    "MAX_TRAIN_SAMPLES = None\n",
    "\n",
    "\n",
    "# ---------- Simple SARI implementation (no easse) ----------\n",
    "\n",
    "def _get_ngrams(tokens: List[str], n: int):\n",
    "    return [\" \".join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "def _sari_sentence(src: str, cand: str, refs: List[str], max_n: int = 4) -> float:\n",
    "    src_toks = src.split()\n",
    "    cand_toks = cand.split()\n",
    "    refs_toks = [r.split() for r in refs]\n",
    "\n",
    "    F_add_total = F_keep_total = F_del_total = 0.0\n",
    "    n_count = 0\n",
    "\n",
    "    for n in range(1, max_n+1):\n",
    "        n_count += 1\n",
    "        src_ngrams = set(_get_ngrams(src_toks, n))\n",
    "        cand_ngrams = set(_get_ngrams(cand_toks, n))\n",
    "        refs_ngrams_list = [set(_get_ngrams(rt, n)) for rt in refs_toks]\n",
    "\n",
    "        # Added\n",
    "        cand_add = cand_ngrams - src_ngrams\n",
    "        refs_add = set().union(*[(r - src_ngrams) for r in refs_ngrams_list]) if refs_ngrams_list else set()\n",
    "        overlap_add = cand_add & refs_add\n",
    "        P_add = len(overlap_add) / len(cand_add) if cand_add else 0.0\n",
    "        R_add = len(overlap_add) / len(refs_add) if refs_add else 0.0\n",
    "        F_add = 2 * P_add * R_add / (P_add + R_add) if (P_add + R_add) > 0 else 0.0\n",
    "\n",
    "        # Kept\n",
    "        cand_keep = cand_ngrams & src_ngrams\n",
    "        refs_keep = set().union(*[(r & src_ngrams) for r in refs_ngrams_list]) if refs_ngrams_list else set()\n",
    "        overlap_keep = cand_keep & refs_keep\n",
    "        P_keep = len(overlap_keep) / len(cand_keep) if cand_keep else 0.0\n",
    "        R_keep = len(overlap_keep) / len(refs_keep) if refs_keep else 0.0\n",
    "        F_keep = 2 * P_keep * R_keep / (P_keep + R_keep) if (P_keep + R_keep) > 0 else 0.0\n",
    "\n",
    "        # Deleted\n",
    "        src_del = src_ngrams - cand_ngrams\n",
    "        refs_del = set().union(*[(src_ngrams - r) for r in refs_ngrams_list]) if refs_ngrams_list else set()\n",
    "        overlap_del = src_del & refs_del\n",
    "        P_del = len(overlap_del) / len(src_del) if src_del else 0.0\n",
    "        R_del = len(overlap_del) / len(refs_del) if refs_del else 0.0\n",
    "        F_del = 2 * P_del * R_del / (P_del + R_del) if (P_del + R_del) > 0 else 0.0\n",
    "\n",
    "        F_add_total += F_add\n",
    "        F_keep_total += F_keep\n",
    "        F_del_total += F_del\n",
    "\n",
    "    F_add_avg = F_add_total / n_count\n",
    "    F_keep_avg = F_keep_total / n_count\n",
    "    F_del_avg = F_del_total / n_count\n",
    "\n",
    "    return (F_add_avg + F_keep_avg + F_del_avg) / 3.0\n",
    "\n",
    "def sari_corpus(sources: List[str], candidates: List[str], references: List[List[str]]) -> float:\n",
    "    scores = [\n",
    "        _sari_sentence(s, c, rs)\n",
    "        for s, c, rs in zip(sources, candidates, references)\n",
    "    ]\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "\n",
    "# ---------- Load WikiLarge ----------\n",
    "\n",
    "def read_parallel(src_path, dst_path, filter_short=True, filter_ratio=True):\n",
    "    with open(src_path, \"r\", encoding=\"utf-8\") as f_src, \\\n",
    "         open(dst_path, \"r\", encoding=\"utf-8\") as f_dst:\n",
    "        src_lines = [line.strip() for line in f_src.readlines()]\n",
    "        dst_lines = [line.strip() for line in f_dst.readlines()]\n",
    "\n",
    "    assert len(src_lines) == len(dst_lines)\n",
    "    src_out, dst_out = [], []\n",
    "\n",
    "    for s, d in zip(src_lines, dst_lines):\n",
    "        if not s or not d:\n",
    "            continue\n",
    "        if filter_short and (len(s.split()) < 5 or len(d.split()) < 3):\n",
    "            continue\n",
    "        if filter_ratio:\n",
    "            # keep only cases where target is shorter than source\n",
    "            if len(d.split()) >= len(s.split()):\n",
    "                continue\n",
    "            ratio = len(d.split()) / len(s.split())\n",
    "            if ratio >= 1:\n",
    "                continue\n",
    "\n",
    "        src_out.append(s)\n",
    "        dst_out.append(d)\n",
    "\n",
    "    # NOTE: columns are named \"source\" and \"target\"\n",
    "    return {\"source\": src_out, \"target\": dst_out}\n",
    "\n",
    "def load_wikilarge(data_dir):\n",
    "    train = read_parallel(\n",
    "        os.path.join(data_dir, \"wiki.full.aner.train.src\"),\n",
    "        os.path.join(data_dir, \"wiki.full.aner.train.dst\"),\n",
    "        filter_short=True,\n",
    "        filter_ratio=True\n",
    "    )\n",
    "    valid = read_parallel(\n",
    "        os.path.join(data_dir, \"wiki.full.aner.valid.src\"),\n",
    "        os.path.join(data_dir, \"wiki.full.aner.valid.dst\"),\n",
    "        filter_short=True,\n",
    "        filter_ratio=True\n",
    "    )\n",
    "    test = read_parallel(\n",
    "        os.path.join(data_dir, \"wiki.full.aner.test.src\"),\n",
    "        os.path.join(data_dir, \"wiki.full.aner.test.dst\"),\n",
    "        filter_short=False,\n",
    "        filter_ratio=False\n",
    "    )\n",
    "\n",
    "    ds_train = Dataset.from_dict(train)\n",
    "    ds_valid = Dataset.from_dict(valid)\n",
    "    ds_test = Dataset.from_dict(test)\n",
    "\n",
    "    return DatasetDict(train=ds_train, validation=ds_valid, test=ds_test)\n",
    "\n",
    "raw_datasets = load_wikilarge(DATA_DIR)\n",
    "\n",
    "\n",
    "# ---------- Tokenizer + model ----------\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def preprocess(examples):\n",
    "    # use correct column names: \"source\" and \"target\"\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"source\"],\n",
    "        max_length=MAX_SOURCE_LENGTH,\n",
    "        truncation=True,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[\"target\"],\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        truncation=True,\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=[\"source\", \"target\"],\n",
    ")\n",
    "\n",
    "print(tokenized_datasets)\n",
    "\n",
    "\n",
    "# ---------- Manual training loop (subset) ----------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "model.to(device)\n",
    "\n",
    "train_dataset_tok = tokenized_datasets[\"train\"]\n",
    "if MAX_TRAIN_SAMPLES is not None and MAX_TRAIN_SAMPLES < len(train_dataset_tok):\n",
    "    train_dataset_tok = train_dataset_tok.select(range(MAX_TRAIN_SAMPLES))\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_tok,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "print(\"Start training on subset:\", len(train_dataset_tok), \"examples\")\n",
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "    for batch in epoch_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "\n",
    "# ---------- Evaluation: BLEU + SARI (with tqdm) ----------\n",
    "\n",
    "print(\"Evaluating on test set (BLEU + SARI)...\")\n",
    "test_dataset_tok = tokenized_datasets[\"test\"]\n",
    "test_raw = raw_datasets[\"test\"]\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset_tok,\n",
    "    batch_size=EVAL_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "pred_texts = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Generating\", leave=False):\n",
    "        batch = {\n",
    "            k: v.to(device)\n",
    "            for k, v in batch.items()\n",
    "            if k in [\"input_ids\", \"attention_mask\"]\n",
    "        }\n",
    "        outputs = model.generate(\n",
    "            **batch,\n",
    "            max_length=MAX_TARGET_LENGTH,\n",
    "            num_beams=4,\n",
    "        )\n",
    "        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        pred_texts.extend(decoded)\n",
    "\n",
    "# Align lengths using correct raw keys\n",
    "test_src = test_raw[\"source\"][: len(pred_texts)]\n",
    "test_ref = test_raw[\"target\"][: len(pred_texts)]\n",
    "\n",
    "# BLEU (0–100)\n",
    "bleu = sacrebleu.corpus_bleu(pred_texts, [test_ref]).score\n",
    "\n",
    "# SARI (our implementation is 0–1; convert to 0–100 for reporting)\n",
    "sari_raw = sari_corpus(\n",
    "    test_src,\n",
    "    pred_texts,\n",
    "    [[r] for r in test_ref],\n",
    ")\n",
    "sari = sari_raw * 100.0\n",
    "\n",
    "print(f\"Test BLEU: {bleu:.2f}\")\n",
    "print(f\"Test SARI: {sari:.2f}\")\n",
    "\n",
    "\n",
    "# ---------- Save outputs to OUTPUT_DIR ----------\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 1) Save model + tokenizer\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# 2) Save metrics\n",
    "metrics = {\"bleu\": float(bleu), \"sari\": float(sari)}\n",
    "with open(os.path.join(OUTPUT_DIR, \"metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "# 3) Save predictions for error analysis\n",
    "pred_path = os.path.join(OUTPUT_DIR, \"predictions.tsv\")\n",
    "with open(pred_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"src\\tref\\tpred\\n\")\n",
    "    for s, r, p in zip(test_src, test_ref, pred_texts):\n",
    "        s_clean = s.replace(\"\\t\", \" \")\n",
    "        r_clean = r.replace(\"\\t\", \" \")\n",
    "        p_clean = p.replace(\"\\t\", \" \")\n",
    "        f.write(f\"{s_clean}\\t{r_clean}\\t{p_clean}\\n\")\n",
    "\n",
    "print(\"Saved model and outputs to:\", OUTPUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
